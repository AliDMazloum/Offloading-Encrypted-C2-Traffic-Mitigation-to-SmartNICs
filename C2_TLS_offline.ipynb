{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2cb74dcd-7be5-4c67-a13e-58628e936ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "89057a7e-434d-406d-92d5-230b60cc3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory_tls12 = \"/home/admin/Test_dir/datasets/ms/TLS1.2/short_cert/\"\n",
    "log_files12 = [f for f in os.listdir(log_directory_tls12) if f.endswith('.log')]\n",
    "\n",
    "log_directory_tls13 = \"/home/admin/Test_dir/datasets/ms/TLS1.3/\"\n",
    "log_files13 = [f for f in os.listdir(log_directory_tls13) if f.endswith('.log')]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for log_file in log_files12:\n",
    "    file_path = os.path.join(log_directory_tls12, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "for log_file in log_files13:\n",
    "    file_path = os.path.join(log_directory_tls13, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "\n",
    "headers = ['client_hello_len','client_hello_ext_num','server_hello_len','server_hello_ext_num','tls_version']\n",
    "df_ms = pd.concat(df_list, ignore_index=True)\n",
    "df_ms.shape\n",
    "df_ms = shuffle(df_ms, random_state=42)\n",
    "df_ms.reset_index(drop=True, inplace=True)\n",
    "# df_ms = df_ms.iloc[:40000]\n",
    "df_ms = df_ms.set_axis(headers, axis=1)\n",
    "df_ms['c2'] = 1\n",
    "df_ms['source'] = 'MS'\n",
    "df_ms_noise = df_ms.iloc[40001:42000]\n",
    "df_ms = df_ms.iloc[:40000]\n",
    "df_ms_noise.loc[:, 'c2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e1f71e45-4488-4e2d-bc45-7f987e3a3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = \"/home/admin/Test_dir/datasets/tranco\"\n",
    "\n",
    "# List all .log files in the directory\n",
    "log_files = [f for f in os.listdir(log_directory) if f.endswith('.log')]\n",
    "\n",
    "# List to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each log file and load it into a DataFrame\n",
    "for log_file in log_files:\n",
    "    file_path = os.path.join(log_directory, log_file)\n",
    "    \n",
    "    # Read the log file into a DataFrame (assuming CSV or tab-delimited format)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)  # or change delimiter depending on the log format\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    # Remove rows where any column contains NaN (or previously \"-\")\n",
    "    df = df.dropna()\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_tranco= pd.concat(df_list, ignore_index=True)\n",
    "df_tranco = shuffle(df_tranco, random_state=42)\n",
    "df_tranco.reset_index(drop=True, inplace=True)\n",
    "df_tranco = df_tranco.iloc[:100000]\n",
    "df_tranco = df_tranco.set_axis(headers, axis=1)\n",
    "# df_tranco = df_tranco[headers[0:3]]\n",
    "# df_tranco = df_tranco[['tls_b_0','tls_b_1','tls_b_5','tls_b_7']]\n",
    "df_tranco['c2'] = 0\n",
    "df_tranco['source'] = 'tranco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7b5ce0ab-2d37-450e-9d64-23a642bf3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory_benign = \"/home/admin/Test_dir/datasets/DoHBrw/benign\"\n",
    "\n",
    "# List all .log files in the directory\n",
    "log_files_benign = [f for f in os.listdir(log_directory_benign) if f.endswith('.log')]\n",
    "\n",
    "# List to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each log file and load it into a DataFrame\n",
    "for log_file in log_files_benign:\n",
    "    file_path = os.path.join(log_directory_benign, log_file)\n",
    "    \n",
    "    # Read the log file into a DataFrame (assuming CSV or tab-delimited format)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)  # or change delimiter depending on the log format\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    # Remove rows where any column contains NaN (or previously \"-\")\n",
    "    df = df.dropna()\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_doh_benign= pd.concat(df_list, ignore_index=True)\n",
    "df_doh_benign = shuffle(df_doh_benign, random_state=42)\n",
    "df_doh_benign.reset_index(drop=True, inplace=True)\n",
    "df_doh_benign = df_doh_benign.iloc[:20000]\n",
    "df_doh_benign = df_doh_benign.set_axis(headers, axis=1)\n",
    "df_doh_benign['c2'] = 0\n",
    "df_doh_benign['source'] = 'DoH'\n",
    "###########################################################################################################################\n",
    "log_directory_malicious = \"/home/admin/Test_dir/datasets/DoHBrw/malicious/\"\n",
    "\n",
    "# List all .log files in the directory\n",
    "log_files_malicious = [f for f in os.listdir(log_directory_malicious) if f.endswith('.log')]\n",
    "\n",
    "# List to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each log file and load it into a DataFrame\n",
    "for log_file in log_files_malicious:\n",
    "    file_path = os.path.join(log_directory_malicious, log_file)\n",
    "    \n",
    "    # Read the log file into a DataFrame (assuming CSV or tab-delimited format)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)  # or change delimiter depending on the log format\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    # Remove rows where any column contains NaN (or previously \"-\")\n",
    "    df = df.dropna()\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_doh_malicious= pd.concat(df_list, ignore_index=True)\n",
    "df_doh_malicious = shuffle(df_doh_malicious, random_state=42)\n",
    "df_doh_malicious.reset_index(drop=True, inplace=True)\n",
    "df_doh_malicious = df_doh_malicious.iloc[:20000]\n",
    "df_doh_malicious = df_doh_malicious.set_axis(headers, axis=1)\n",
    "df_doh_malicious['c2'] = 1\n",
    "df_doh_malicious['source'] = 'DoH'\n",
    "\n",
    "df_doh = pd.concat([df_doh_malicious,df_doh_benign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "3fd892f5-bb97-4a5e-b98c-7b2f74aeacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Load MTA dataset (i.e., Malicious and Benign traffic)\n",
    "# df_mta = pd.read_csv('/home/admin/extending12to13/processed-datasets/labeled_mta.csv', low_memory=False)\n",
    "# columns = ['tls_b_0','tls_b_1','tls_b_5','tls_b_7'] + [\"c2\"]\n",
    "# df_mta = df_mta[columns]\n",
    "# # df_mta = df_mta[df_mta[\"tls_b_9\"]>0]\n",
    "# df_mta['source'] = \"MTA\"\n",
    "# df_mta.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "807c3644-5f0c-4c5c-ad86-1b1fb22c532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_tranco,df_ms,df_ms_noise, df_doh], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "1c6857aa-33b2-4d3f-8cb9-7429162ba2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = shuffle(df_all, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "25a31492-35e1-4575-ab22-e9a1b10ab351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06deb37-18a2-4a14-83da-a0f0fc40afbf",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8a804b0e-d0dd-4086-982b-15e4148549d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_all.loc[:,headers].values.astype(np.float64)\n",
    "y = df_all.loc[:,\"c2\"].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "19a2740e-ecdc-4a75-ad17-6a9018f86a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y , \n",
    "                                   random_state=42,  \n",
    "                                   test_size=0.25,  \n",
    "                                   shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5ffbb-a2df-4c6e-9fe1-3743d55bb96f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "608fa0c0-7699-4d5f-a4d9-a04dd5045b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\"AUC\": \"roc_auc\",\n",
    "           \"Accuracy\": \"accuracy\",\n",
    "           \"Balanced Accracy\": \"balanced_accuracy\",\n",
    "           \"Recall\": \"recall\",\n",
    "           \"Neg Recall\": make_scorer(recall_score, pos_label=0),\n",
    "           \"F1\": \"f1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "b09738bf-8bbc-4008-8246-bf3e4f3a8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_classifier = dtc(random_state=42)\n",
    "parameters = {'max_depth':list(range(1,20))+[30,40,50,60], \n",
    "              'criterion': ['gini', 'entropy'], \n",
    "              'splitter': ['best', 'random'], \n",
    "              'max_features': [None, 'sqrt', 'log2'],\n",
    "              'class_weight': [None, 'balanced']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "989afff3-ec19-46cb-b79d-79c0e7a05528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(estimator=dtc_classifier, param_grid=parameters, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# tree_model = grid_search.best_estimator_\n",
    "# tree_model.fit(X_train, y_train)\n",
    "# y_pred = tree_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test data: {accuracy}\")\n",
    "# importances = tree_model.feature_importances_\n",
    "# feature_importances = pd.DataFrame({'feature': headers, 'importance': importances})\n",
    "# feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "# feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "79435ce2-e1ae-44f6-bd95-1b58b7d9cd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9862043035631652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_hello_ext_num</td>\n",
       "      <td>0.513539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_hello_len</td>\n",
       "      <td>0.210550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>server_hello_ext_num</td>\n",
       "      <td>0.209984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tls_version</td>\n",
       "      <td>0.059575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>server_hello_len</td>\n",
       "      <td>0.006352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature  importance\n",
       "1  client_hello_ext_num    0.513539\n",
       "0      client_hello_len    0.210550\n",
       "3  server_hello_ext_num    0.209984\n",
       "4           tls_version    0.059575\n",
       "2      server_hello_len    0.006352"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "rf_model = RandomForestClassifier(max_depth=20, n_estimators = 5, max_leaf_nodes=500, n_jobs=4, random_state=42, bootstrap=False)                              \n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test data: {accuracy}\")\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': headers, 'importance': importances})\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7791bded-9d86-428c-908a-9b80bb5a9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(rf_model, open(\"RF.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c4557-6fa6-4ae0-be5c-3cec2b39fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
