{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cb74dcd-7be5-4c67-a13e-58628e936ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ba5bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory_tls12 = \"/home/admin/C2_TLS/datasets/ms/TLS1.2/short_cert/zeek_logs\"\n",
    "log_files12 = [f for f in os.listdir(log_directory_tls12) if f.endswith('.log')]\n",
    "\n",
    "log_directory_tls13 = \"/home/admin/C2_TLS/datasets/ms/TLS1.3/zeek_logs\"\n",
    "log_files13 = [f for f in os.listdir(log_directory_tls13) if f.endswith('.log')]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for log_file in log_files12:\n",
    "    file_path = os.path.join(log_directory_tls12, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "for log_file in log_files13:\n",
    "    file_path = os.path.join(log_directory_tls13, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "\n",
    "headers = ['client_hello_len','client_hello_ext_num','server_hello_len','server_hello_ext_num']\n",
    "df_ms = pd.concat(df_list, ignore_index=True)\n",
    "df_ms.shape\n",
    "df_ms = shuffle(df_ms, random_state=42)\n",
    "df_ms.reset_index(drop=True, inplace=True)\n",
    "df_ms = df_ms.set_axis(headers, axis=1)\n",
    "df_ms['c2'] = 1\n",
    "df_ms['source'] = 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d43ef45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory_tls12 = \"/home/admin/C2_TLS/datasets/tranco/TLS1.2/zeek_logs\"\n",
    "log_files12 = [f for f in os.listdir(log_directory_tls12) if f.endswith('.log')]\n",
    "\n",
    "log_directory_tls13 = \"/home/admin/C2_TLS/datasets/tranco/TLS1.3/zeek_logs\"\n",
    "log_files13 = [f for f in os.listdir(log_directory_tls13) if f.endswith('.log')]\n",
    "\n",
    "# List to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "for log_file in log_files12:\n",
    "    file_path = os.path.join(log_directory_tls12, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "for log_file in log_files13:\n",
    "    file_path = os.path.join(log_directory_tls13, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_tranco= pd.concat(df_list, ignore_index=True)\n",
    "df_tranco = shuffle(df_tranco, random_state=42)\n",
    "df_tranco.reset_index(drop=True, inplace=True)\n",
    "df_tranco = df_tranco.set_axis(headers, axis=1)\n",
    "df_tranco['c2'] = 0\n",
    "df_tranco['source'] = 'tranco'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6560138",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory_benign = \"/home/admin/C2_TLS/datasets/DoHBrw/benign/zeek_logs\"\n",
    "\n",
    "log_files_benign = [f for f in os.listdir(log_directory_benign) if f.endswith('.log')]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for log_file in log_files_benign:\n",
    "    file_path = os.path.join(log_directory_benign, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)  # or change delimiter depending on the log format\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_doh_benign= pd.concat(df_list, ignore_index=True)\n",
    "df_doh_benign = shuffle(df_doh_benign, random_state=42)\n",
    "df_doh_benign.reset_index(drop=True, inplace=True)\n",
    "df_doh_benign = df_doh_benign.set_axis(headers, axis=1)\n",
    "df_doh_benign['c2'] = 0\n",
    "df_doh_benign['source'] = 'DoH'\n",
    "###########################################################################################################################\n",
    "log_directory_malicious = \"/home/admin/C2_TLS/datasets/DoHBrw/malicious/zeek_logs\"\n",
    "\n",
    "log_files_malicious = [f for f in os.listdir(log_directory_malicious) if f.endswith('.log')]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for log_file in log_files_malicious:\n",
    "    file_path = os.path.join(log_directory_malicious, log_file)\n",
    "    df = pd.read_csv(file_path, sep=\",\", header=None, skiprows=8, low_memory=False)  # or change delimiter depending on the log format\n",
    "    df = df.iloc[:-1]\n",
    "    df.replace(\"-\", pd.NA, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df_list.append(df)\n",
    "\n",
    "#Load Tranco dataset (i.e., Benign traffic)\n",
    "df_doh_malicious= pd.concat(df_list, ignore_index=True)\n",
    "df_doh_malicious = shuffle(df_doh_malicious, random_state=42)\n",
    "df_doh_malicious.reset_index(drop=True, inplace=True)\n",
    "df_doh_malicious = df_doh_malicious.set_axis(headers, axis=1)\n",
    "df_doh_malicious['c2'] = 1\n",
    "df_doh_malicious['source'] = 'DoH'\n",
    "\n",
    "df_doh = pd.concat([df_doh_malicious,df_doh_benign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "807c3644-5f0c-4c5c-ad86-1b1fb22c532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_hello_len</th>\n",
       "      <th>client_hello_ext_num</th>\n",
       "      <th>server_hello_len</th>\n",
       "      <th>server_hello_ext_num</th>\n",
       "      <th>c2</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>589</td>\n",
       "      <td>14.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tranco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>508</td>\n",
       "      <td>12.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>DoH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>10.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>508</td>\n",
       "      <td>10.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391932</th>\n",
       "      <td>580</td>\n",
       "      <td>14.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tranco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391933</th>\n",
       "      <td>587</td>\n",
       "      <td>14.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tranco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391934</th>\n",
       "      <td>182</td>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tranco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391935</th>\n",
       "      <td>508</td>\n",
       "      <td>10.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391936</th>\n",
       "      <td>187</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tranco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1391937 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        client_hello_len  client_hello_ext_num  server_hello_len  \\\n",
       "0                    589                  14.0             118.0   \n",
       "1                    508                  12.0              94.0   \n",
       "2                    252                  10.0             118.0   \n",
       "3                    508                  10.0             118.0   \n",
       "4                    179                   6.0              61.0   \n",
       "...                  ...                   ...               ...   \n",
       "1391932              580                  14.0             118.0   \n",
       "1391933              587                  14.0             118.0   \n",
       "1391934              182                  10.0              76.0   \n",
       "1391935              508                  10.0             118.0   \n",
       "1391936              187                  10.0              74.0   \n",
       "\n",
       "         server_hello_ext_num  c2  source  \n",
       "0                         2.0   0  tranco  \n",
       "1                         3.0   0     DoH  \n",
       "2                         2.0   1     DoH  \n",
       "3                         2.0   1      MS  \n",
       "4                         4.0   1      MS  \n",
       "...                       ...  ..     ...  \n",
       "1391932                   2.0   0  tranco  \n",
       "1391933                   2.0   0  tranco  \n",
       "1391934                   7.0   0  tranco  \n",
       "1391935                   2.0   1      MS  \n",
       "1391936                   6.0   0  tranco  \n",
       "\n",
       "[1391937 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([df_tranco,df_ms, df_doh], ignore_index=True)\n",
    "df_all = shuffle(df_all, random_state=42)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06deb37-18a2-4a14-83da-a0f0fc40afbf",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a804b0e-d0dd-4086-982b-15e4148549d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_all.loc[:,headers].values.astype(np.float64)\n",
    "y = df_all.loc[:,\"c2\"].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a2740e-ecdc-4a75-ad17-6a9018f86a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y , \n",
    "                                   random_state=42,  \n",
    "                                   test_size=0.25,  \n",
    "                                   shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5ffbb-a2df-4c6e-9fe1-3743d55bb96f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79435ce2-e1ae-44f6-bd95-1b58b7d9cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_hello_ext_num</td>\n",
       "      <td>0.519459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>server_hello_len</td>\n",
       "      <td>0.210499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_hello_len</td>\n",
       "      <td>0.137703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>server_hello_ext_num</td>\n",
       "      <td>0.132338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature  importance\n",
       "1  client_hello_ext_num    0.519459\n",
       "2      server_hello_len    0.210499\n",
       "0      client_hello_len    0.137703\n",
       "3  server_hello_ext_num    0.132338"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(max_depth=20, n_estimators = 5, max_leaf_nodes=500, n_jobs=4, random_state=42, bootstrap=False)                              \n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test data: {accuracy}\")\n",
    "# print(classification_report(y_test,y_pred))\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': headers, 'importance': importances})\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791bded-9d86-428c-908a-9b80bb5a9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(rf_model, open(\"RF_DPDK.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe2ed7",
   "metadata": {},
   "source": [
    "# Transform RF model into Json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8143ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model saved to rf_model.json\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Function to extract information from each tree in the Random Forest\n",
    "def tree_to_json(tree):\n",
    "    # Get the decision tree's internal attributes\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "    class_names = tree.classes_\n",
    "\n",
    "    nodes = []\n",
    "    for i in range(n_nodes):\n",
    "        node = {}\n",
    "        # If it's a leaf node, assign class label\n",
    "        if children_left[i] == children_right[i]:\n",
    "            node['is_leaf'] = True\n",
    "            # Extract class counts/probabilities and assign the class with the max count/probability\n",
    "            class_counts = tree.tree_.value[i, 0]\n",
    "            class_label = class_counts.argmax()  # Get the index of the maximum count\n",
    "            node['class_label'] = int(class_names[class_label])  # Get the corresponding class label\n",
    "        else:\n",
    "            node['is_leaf'] = False\n",
    "            node['class_label'] = -10\n",
    "\n",
    "        node['feature'] = feature[i] if feature[i] != -2 else -2  # -2 indicates no split (leaf)\n",
    "        node['threshold'] = threshold[i]\n",
    "        node['left_child'] = children_left[i]\n",
    "        node['right_child'] = children_right[i]\n",
    "        \n",
    "        nodes.append(node)\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "# Function to convert the Random Forest model to JSON format\n",
    "def rf_to_json(rf_model):\n",
    "    # print(rf_model)\n",
    "    rf_data = {\n",
    "        'n_estimators': len(rf_model.estimators_),\n",
    "        'max_depth': rf_model.max_depth,\n",
    "        'feature_importances': rf_model.feature_importances_.tolist(),\n",
    "        'estimators': []\n",
    "    }\n",
    "\n",
    "    # Convert each estimator (tree) to JSON\n",
    "    for estimator in rf_model.estimators_:\n",
    "        # For each tree (estimator), get the nodes and relevant details\n",
    "        estimators_data = {\n",
    "            'n_nodes': estimator.tree_.node_count,\n",
    "            'children_left': estimator.tree_.children_left.tolist(),\n",
    "            'children_right': estimator.tree_.children_right.tolist(),\n",
    "            'feature': estimator.tree_.feature.tolist(),\n",
    "            'threshold': estimator.tree_.threshold.tolist(),\n",
    "            'class_label': [] , # Initialize a list for class labels\n",
    "            'leaves' : []\n",
    "        }\n",
    "        # Extract class labels from the tree's value attribute\n",
    "        for i in range(estimator.tree_.node_count):\n",
    "            if estimator.tree_.children_left[i] == estimator.tree_.children_right[i]:\n",
    "                estimators_data['leaves'].append(1)\n",
    "                class_counts = estimator.tree_.value[i, 0]  # Get the counts for each class at the leaf node\n",
    "                max_class_index = class_counts.argmax()  # Get the index of the max class count\n",
    "                estimators_data['class_label'].append(int(estimator.classes_[max_class_index]))  # Add the class label\n",
    "            else:\n",
    "                estimators_data['leaves'].append(0)\n",
    "                estimators_data['class_label'].append(-10)  # Add the class label\n",
    "\n",
    "        rf_data['estimators'].append(estimators_data)\n",
    "\n",
    "    return rf_data\n",
    "\n",
    "# Load the Random Forest model from the .pkl file\n",
    "with open('RF_DPDK.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "# Convert the model to JSON\n",
    "rf_json = rf_to_json(rf_model)\n",
    "\n",
    "# Write the JSON to a file\n",
    "with open('rf_model.json', 'w') as json_file:\n",
    "    json.dump(rf_json, json_file, indent=4)\n",
    "\n",
    "print(\"Random Forest model saved to rf_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a17f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
